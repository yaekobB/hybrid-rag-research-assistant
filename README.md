# LLM-Powered Research Assistant

A modular, RAG-based **research assistant** that combines:

- Local **Retrieval-Augmented Generation (RAG)** over user-uploaded PDFs  
- **Web RAG** using Tavily for fresh external knowledge  
- A simple **task-type planner** (QA, summary, compare, plan, survey)  
- A structured **Writer** that produces academic-style answers  
- A **Reflection** module that critiques and improves those answers  
- A Streamlit **web UI** with LLM parameter control and PDF export

This project was developed to demonstrate
multi-step reasoning, contextual grounding, and transparent, research-style outputs.

---

## 1. Features

- ğŸ” **Local RAG over PDFs**
  - Upload course papers, survey articles, or project documents.
  - PDFs are chunked, embedded (`all-MiniLM-L6-v2`), and stored in an in-memory vector store.
  - Semantic search retrieves the most relevant chunks as context.

- ğŸŒ **Web RAG (Tavily API)**
  - Uses Tavily to fetch up-to-date web evidence for queries that go beyond local PDFs.
  - Web results are turned into numbered context snippets with URLs for citation.

- ğŸ§  **Task-Type Planner**
  - Simple rule-based classifier that maps each query to a task type:
    - `qa` â€“ academic-style explanation
    - `summary` â€“ concise summary
    - `compare` â€“ comparison of methods/approaches
    - `plan` â€“ high-level research/project plan
    - `survey` â€“ mini literature-style overview

- âœï¸ **Writer Module (Ollama Llama 3.2)**
  - Generates structured answers using task-specific templates:
    - 5-section QA (Introduction, Key Concepts, Evidence, Limitations, Conclusion)
    - Summary, comparison, research plan, or survey structures.
  - Uses inline numeric citations like `[1]`, `[2]` tied to retrieved evidence.

- ğŸ” **Reflection Module**
  - A second LLM pass that:
    - Critiques the draft answer (clarity, grounding, hallucinations).
    - Produces an **Improved Answer** while keeping the same structure.
  - The **final answer** is the improved one; draft and critique are visible in debug mode.

- ğŸ–¥ï¸ **Streamlit UI**
  - Text box for research questions.
  - PDF upload + ingestion controls.
  - Source selection: `local`, `web`, `hybrid`.
  - **LLM parameters in the UI**:
    - `temperature` (slider)
    - `max_tokens` (numeric input)
  - Answer panel + Reference list.
  - Debug panel (optional) with:
    - Task type
    - Draft answer
    - Reflection critique
    - Raw retrieval results
  - â€œDownload as PDFâ€ button (Question + Answer + References).

---

## 2. Project Structure

Below is the **typical folder structure** of the project (you can adapt names if needed):

```text
project-root/
â”œâ”€ app/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ config.py                # Paths, model name, Tavily key, etc.
â”‚  â”œâ”€ logging_config.py        # Central logging setup
â”‚  â”‚
â”‚  â”œâ”€ pipeline/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â””â”€ orchestrator.py       # Main pipeline: planning, retrieval, writer, reflection
â”‚  â”‚
â”‚  â”œâ”€ tools/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ local_rag_langchain.py  # LangChain PDF ingestion + semantic search
â”‚  â”‚  â”œâ”€ rag_web.py              # Tavily-based web retrieval
â”‚  â”‚  â””â”€ export_utils.py         # Helper for exporting answers to PDF
â”‚  â”‚
â”‚  â””â”€ ui/
â”‚     â”œâ”€ __init__.py
â”‚     â””â”€ ui_streamlit.py       # Streamlit front-end (tabs, controls, debug)
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ pdfs_uploaded/           # Uploaded PDFs (input)
â”‚  â””â”€ vectorstore/             # (Optional) persistent embeddings/index
â”‚
â”œâ”€ logs/                       # Application logs (optional)
â”‚
â”œâ”€ requirements.txt            # Python dependencies
â”œâ”€ README.md                   # Project documentation
â””â”€ .env.example                # Example for environment variables (Tavily API key, etc.)
```


---

## 3. Installation

1. **Clone the repository**

```bash
git clone https://github.com/yaekobB/hybrid-rag-research-assistant.git
cd hybrid-rag-research-assistant
```

2. **Create and activate a virtual environment** (recommended)

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux / macOS
source .venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Install and pull the LLM with Ollama**

- Install Ollama from: https://ollama.com  
- Pull the model used in this project (for example):
```bash
ollama pull llama3.2
```

5. **Set Tavily API key (if using Web RAG)**

Create a `.env` file or set an environment variable:

```bash
export TAVILY_API_KEY="your_tavily_api_key_here"
```

On Windows (PowerShell):

```powershell
$env:TAVILY_API_KEY="your_tavily_api_key_here"
```

Make sure `config.py` reads this key correctly.

---

## 4. How to Run

1. Start the **Ollama** server (if not already running):
```bash
ollama serve
```

2. From the project root, launch the **Streamlit app**:

```bash
python -m streamlit run app/ui/ui_streamlit.py
```

3. Open the URL shown in the terminal (e.g. `http://localhost:8501`).

---

## 5. Usage Guide

1. **Upload PDFs (optional but recommended)**
   - Use the sidebar â€œUpload PDFsâ€ area.
   - Click *â€œIngest PDFs into Local RAG (LangChain)â€* to index them.

2. **Choose source mode**
   - `local`  â€“ only your uploaded PDFs.
   - `web`    â€“ only Tavily web search.
   - `hybrid` â€“ combine both.

3. **Tune LLM parameters in the UI**
   - **Temperature**: lower = more deterministic, higher = more creative.
   - **Max tokens**: upper bound on answer length.

4. **Ask your research question**
   - Examples:
     - *â€œWhat are recent research directions in AI for medical imaging?â€*
     - *â€œSummarize this paper in 5 bullet points.â€*
     - *â€œCompare RAG and Self-RAG for factuality.â€*
     - *â€œGive me a research plan for a thesis on chest X-ray report generation.â€*

5. **Inspect the output**
   - Main panel: final, **refined** answer (after reflection).
   - References: numbered evidence sources (PDFs and/or web URLs).
   - Debug mode (checkbox in sidebar):
     - Shows *task type*, *draft answer*, *reflection critique*, and *raw retrieval*.

6. **Export as PDF**
   - Use the *â€œDownload Answer as PDFâ€* button to save a clean report containing:
     - Question
     - Final answer
     - References

---

## 6. Debugging & Troubleshooting

- **No answer / error during retrieval**
  - Check that PDFs were ingested (for `local` mode).
  - Check your Tavily API key (for `web` or `hybrid` mode).
  - Look at the terminal logs for stack traces.

- **Ollama connection issues**
  - Ensure `ollama serve` is running.
  - Confirm `LLM_MODEL` in `config.py` matches a pulled model (e.g. `llama3.2`).

- **Strange or generic answers**
  - Enable **debug mode** to see:
    - What documents/web pages were retrieved.
    - The draft answer before reflection.
    - The critique, which may reveal missing context or misaligned queries.

---

## 7. Future Improvements

- Learnable task-type classifier (instead of rule-based planner).
- More advanced retrieval (multi-query, reranking, keyword + dense hybrid).
- Richer UI (tabbed chat history, document browser, citation filtering).
- Logging and evaluation scripts for quantitative assessment of answer quality.

---
