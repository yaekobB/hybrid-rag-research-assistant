# LLM-Powered Research Assistant

A modular, exam-ready **research assistant** that combines:

- Local **Retrieval-Augmented Generation (RAG)** over user-uploaded PDFs  
- **Web RAG** using Tavily for fresh external knowledge  
- A simple **task-type planner** (QA, summary, compare, plan, survey)  
- A structured **Writer** that produces academic-style answers  
- A **Reflection** module that critiques and improves those answers  
- A Streamlit **web UI** with LLM parameter control and PDF export

This project was developed for the *Neuro-Symbolic AI Lab* exam and is designed to demonstrate
multi-step reasoning, contextual grounding, and transparent, research-style outputs.

---

## 1. Features

- üîç **Local RAG over PDFs**
  - Upload course papers, survey articles, or project documents.
  - PDFs are chunked, embedded (`all-MiniLM-L6-v2`), and stored in an in-memory vector store.
  - Semantic search retrieves the most relevant chunks as context.

- üåê **Web RAG (Tavily API)**
  - Uses Tavily to fetch up-to-date web evidence for queries that go beyond local PDFs.
  - Web results are turned into numbered context snippets with URLs for citation.

- üß† **Task-Type Planner**
  - Simple rule-based classifier that maps each query to a task type:
    - `qa` ‚Äì academic-style explanation
    - `summary` ‚Äì concise summary
    - `compare` ‚Äì comparison of methods/approaches
    - `plan` ‚Äì high-level research/project plan
    - `survey` ‚Äì mini literature-style overview

- ‚úçÔ∏è **Writer Module (Ollama Llama 3.2)**
  - Generates structured answers using task-specific templates:
    - 5-section QA (Introduction, Key Concepts, Evidence, Limitations, Conclusion)
    - Summary, comparison, research plan, or survey structures.
  - Uses inline numeric citations like `[1]`, `[2]` tied to retrieved evidence.

- üîÅ **Reflection Module**
  - A second LLM pass that:
    - Critiques the draft answer (clarity, grounding, hallucinations).
    - Produces an **Improved Answer** while keeping the same structure.
  - The **final answer** is the improved one; draft and critique are visible in debug mode.

- üñ•Ô∏è **Streamlit UI**
  - Text box for research questions.
  - PDF upload + ingestion controls.
  - Source selection: `local`, `web`, `hybrid`.
  - **LLM parameters in the UI**:
    - `temperature` (slider)
    - `max_tokens` (numeric input)
  - Answer panel + Reference list.
  - Debug panel (optional) with:
    - Task type
    - Draft answer
    - Reflection critique
    - Raw retrieval results
  - ‚ÄúDownload as PDF‚Äù button (Question + Answer + References).

---

## 2. Project Structure

Below is the **typical folder structure** of the project (you can adapt names if needed):

```text
project-root/
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ config.py                # Paths, model name, Tavily key, etc.
‚îÇ  ‚îú‚îÄ logging_config.py        # Central logging setup
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ pipeline/
‚îÇ  ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îÇ  ‚îî‚îÄ orchestrator.py       # Main pipeline: planning, retrieval, writer, reflection
‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ tools/
‚îÇ  ‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îÇ  ‚îú‚îÄ local_rag_langchain.py  # LangChain PDF ingestion + semantic search
‚îÇ  ‚îÇ  ‚îú‚îÄ rag_web.py              # Tavily-based web retrieval
‚îÇ  ‚îÇ  ‚îî‚îÄ export_utils.py         # Helper for exporting answers to PDF
‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ ui/
‚îÇ     ‚îú‚îÄ __init__.py
‚îÇ     ‚îî‚îÄ ui_streamlit.py       # Streamlit front-end (tabs, controls, debug)
‚îÇ
‚îú‚îÄ data/
‚îÇ  ‚îú‚îÄ pdfs_uploaded/           # Uploaded PDFs (input)
‚îÇ  ‚îî‚îÄ vectorstore/             # (Optional) persistent embeddings/index
‚îÇ
‚îú‚îÄ logs/                       # Application logs (optional)
‚îÇ
‚îú‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ README.md                   # Project documentation
‚îî‚îÄ .env.example                # Example for environment variables (Tavily API key, etc.)
```

> ‚ÑπÔ∏è Your local repository may contain additional files (e.g., `.streamlit/`, Docker files, etc.). You can extend this section to match your exact layout.

---

## 3. Installation

1. **Clone the repository**

```bash
git clone <your-repo-url>.git
cd project-root
```

2. **Create and activate a virtual environment** (recommended)

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux / macOS
source .venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Install and pull the LLM with Ollama**

- Install Ollama from: https://ollama.com  
- Pull the model used in this project (for example):
```bash
ollama pull llama3.2
```

5. **Set Tavily API key (if using Web RAG)**

Create a `.env` file or set an environment variable:

```bash
export TAVILY_API_KEY="your_tavily_api_key_here"
```

On Windows (PowerShell):

```powershell
$env:TAVILY_API_KEY="your_tavily_api_key_here"
```

Make sure `config.py` reads this key correctly.

---

## 4. How to Run

1. Start the **Ollama** server (if not already running):
```bash
ollama serve
```

2. From the project root, launch the **Streamlit app**:

```bash
python -m streamlit run app/ui/ui_streamlit.py
```

3. Open the URL shown in the terminal (e.g. `http://localhost:8501`).

---

## 5. Usage Guide

1. **Upload PDFs (optional but recommended)**
   - Use the sidebar ‚ÄúUpload PDFs‚Äù area.
   - Click *‚ÄúIngest PDFs into Local RAG (LangChain)‚Äù* to index them.

2. **Choose source mode**
   - `local`  ‚Äì only your uploaded PDFs.
   - `web`    ‚Äì only Tavily web search.
   - `hybrid` ‚Äì combine both.

3. **Tune LLM parameters in the UI**
   - **Temperature**: lower = more deterministic, higher = more creative.
   - **Max tokens**: upper bound on answer length.

4. **Ask your research question**
   - Examples:
     - *‚ÄúWhat are recent research directions in AI for medical imaging?‚Äù*
     - *‚ÄúSummarize this paper in 5 bullet points.‚Äù*
     - *‚ÄúCompare RAG and Self-RAG for factuality.‚Äù*
     - *‚ÄúGive me a research plan for a thesis on chest X-ray report generation.‚Äù*

5. **Inspect the output**
   - Main panel: final, **refined** answer (after reflection).
   - References: numbered evidence sources (PDFs and/or web URLs).
   - Debug mode (checkbox in sidebar):
     - Shows *task type*, *draft answer*, *reflection critique*, and *raw retrieval*.

6. **Export as PDF**
   - Use the *‚ÄúDownload Answer as PDF‚Äù* button to save a clean report containing:
     - Question
     - Final answer
     - References

---

## 6. Debugging & Troubleshooting

- **No answer / error during retrieval**
  - Check that PDFs were ingested (for `local` mode).
  - Check your Tavily API key (for `web` or `hybrid` mode).
  - Look at the terminal logs for stack traces.

- **Ollama connection issues**
  - Ensure `ollama serve` is running.
  - Confirm `LLM_MODEL` in `config.py` matches a pulled model (e.g. `llama3.2`).

- **Strange or generic answers**
  - Enable **debug mode** to see:
    - What documents/web pages were retrieved.
    - The draft answer before reflection.
    - The critique, which may reveal missing context or misaligned queries.

---

## 7. Future Improvements

- Learnable task-type classifier (instead of rule-based planner).
- More advanced retrieval (multi-query, reranking, keyword + dense hybrid).
- Richer UI (tabbed chat history, document browser, citation filtering).
- Logging and evaluation scripts for quantitative assessment of answer quality.

---

If you use this project for a course exam, thesis, or research prototype, you can easily extend it
with new tools, task types, or evaluation routines while keeping a clear and explainable architecture.
